{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Øving 2, TMA4320\n",
    "\n",
    "* **Veiledning:** Torsdag 16. januar, 0815-1000 i H3\n",
    "* **Innleveringsfrist:** Torsdag 23. januar, kl 1400\n",
    "* **Innleveringsmetode** Følgende to krav er nødvendig for godkjenning\n",
    "    1. Opplasting av Jupyter Notebook (individuelt) i Blackboard\n",
    "    2. Svare på Blackboardskjema for de tre kontrollspørsmålene i øvingen\n",
    "\n",
    "\n",
    "Vi skal i denne oppgaven bruke fikspunktiterasjon og spesifikt Newtons metode, først for skalar ligning og deretter for et system av ligninger.\n",
    "\n",
    "**Oppgave 1** Fikspunktiterasjon for skalare ligninger.\n",
    "Vi ser nå på polynomet\n",
    "\n",
    "$$\n",
    "     p(x) = x^5-3x+1\n",
    "$$\n",
    "som har 3 reelle nullpunkter, $x_1\\approx -1.3887919844072541828$, $x_2\\approx 0.33473414194335268708$\n",
    "og $x_3\\approx 1.2146480426984618040$. Vi bruker to omforminger av ligningen til fikspunktform\n",
    "1. $x=g(x)$ med $g(x)=\\frac13(x^5+1)$ (\"tilfeldig\")\n",
    "2. $x=g(x)$ med $g(x)=x-\\frac{x^5-3x+1}{5x^4-3}$ (Newton)\n",
    "\n",
    "I fikspunktiterasjonen vil vi ha en startverdi $x^{(0)}$ og en toleranse $\\mathrm{tol}$. Stoppkriteriet vi bruker er\n",
    "\n",
    "$$\n",
    "    |x^{(k)}-x^{(k-1)}| < \\mathrm{tol}\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Lag funksjoner *fiksit* og *newton* for hvert av tilfellene tar som input startverdien $x^{(0)}$ og toleransen tol, og returnerer approksimasjon til roten samt antall iterasjoner benyttet. Funksjonene skrives spesifikt for dette eksemplet. Sørg for at funksjonen avslutter dersom metoden ikke har konvergert etter *maxiter* iterasjoner.\n",
    "Test ut med $x^{(0)}=0$ og $\\mathrm{tol}=10^{-10}$.\n",
    "</div>\n",
    "\n",
    "**Kontrollspørsmål 1** Hvor mange iterasjoner bruker de to funksjonene med denne startverdien/toleransen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.33473414194323614, 7)\n",
      "(0.33473414194335266, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def g1(x):\n",
    "    return (x**5 + 1) / 3\n",
    "\n",
    "def g2(x):\n",
    "    return x - (x**5 - 3*x + 1) / (5*x**4 -3)\n",
    "\n",
    "\n",
    "def do_fixit(x0, tol, func, maxits=1E8):\n",
    "    its = 1\n",
    "    x1 = func(x0)\n",
    "    \n",
    "    while abs(x1 - x0) > tol:\n",
    "        assert maxits > its\n",
    "        its +=1\n",
    "        x0, x1 = x1, func(x1)\n",
    "    \n",
    "    return x1, its\n",
    "\n",
    "\n",
    "def fixit(x0,tol):\n",
    "    assert tol>0\n",
    "    return do_fixit(x0, tol, g1)\n",
    "\n",
    "def newton(x0,tol):\n",
    "    assert tol>0\n",
    "    return do_fixit(x0, tol, g2)\n",
    "\n",
    "#Gjør kall til funksjoner som utfører eksperimenter her\n",
    "\n",
    "print(fixit(0, 1E-10))\n",
    "print(newton(0, 1E-10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oppgave 2.** Vi skal nå se på bruk av automatisk derivasjon for Newtons metode. Automatisk derivasjon er en teknikk som ble oppfunnet for ikke så veldig mange år siden. Det er en algoritme som kan ta en vilkårlig funksjon skrevet f.eks. i Python, og genererer fra denne automatisk den deriverte av funksjonen. Det er ikke det samme som symbolsk derivasjon for resultatet foreligger ikke som et symbolsk uttrykk. I dette kurset skal vi kun bruke ferdig implementert automatisk derivasjon. Det fins en pakke for dette formålet i Python som heter *autograd*. Vi kan ikke benytte 'vanlig numpy' når vi gjør autoderivasjon, men bruker en variant autograd.numpy, det er derfor denne vi nå må importere. I vinduet nedenfor demonstrerer vi bruken og har valgt å hente inn funksjonen *value_and_grad* fra *autograd*. Den kan brukes til å finne et tuple bestående av funksjonsverdi og derivert for gitte verdier av argumentet til funksjonen. Mye blir nok klart hvis du leser følgende eksempel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49999999999999994\n",
      "0.8660254037844387\n"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as np   \n",
    "from autograd import value_and_grad \n",
    "\n",
    "def g(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "Dg = value_and_grad(g)\n",
    "G=Dg(np.pi/6.)\n",
    "print(G[0])\n",
    "print(G[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Med $g(x)=\\sin x$ får vi laget funksjonen Dg som returnerer tuplet $(g(x),g'(x))=(\\sin x, \\cos x)$.\n",
    "Som du vet er $\\sin\\frac{\\pi}{6}=\\frac12$ mens $\\cos\\frac{\\pi}{6}=\\frac12\\sqrt{3}\\approx 0.866$.\n",
    "\n",
    "Når man bruker *autograd* er datatypen til input viktig. Du bør for eksempel alltid definere tall som skal være flyttall med desimalpunktum, f.eks. skriv ikke *x0=1*, men *x0=1.0*. I motsatt fall kan du få kryptiske feilmeldinger.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**(a)** Lag en funksjon *gNewton* som tar tre inputargumenter: funksjonen $f(x)$ som en skal finne nullpunkter til, startverdien $x^{(0)}$, og toleransen tol. Returner approksimert rot og antall iterasjoner. </div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**(b)** Test *gNewton* på funksjonen\n",
    "\n",
    "$$\n",
    " f(x) = \\frac13\\sqrt{1+\\sin(\\tanh x)}+\\cos x\n",
    "$$\n",
    "med startverdi $x0=\\{-2.0,2.0,4.0,8.0\\}$, $\\mathrm{tol}=10^{-10}$ og skriv ut rotapproksimasjon og antall iterasjoner i hvert tilfelle. \n",
    "\n",
    "</div>\n",
    "\n",
    "**Kontrollspørsmål 2** Hvor mange distinkte (forskjellige) røtter fikk du konvergens mot totalt med disse 4 startverdiene?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0: (-1.7180861141125476, 4)\n",
      "2.0: (2.0376244457683277, 4)\n",
      "4.0: (4.24303676441417, 5)\n",
      "8.0: (8.323364548870096, 5)\n"
     ]
    }
   ],
   "source": [
    "# Fyll inn koden for b-spørsmålet her\n",
    "import autograd.numpy as np\n",
    "from autograd import value_and_grad\n",
    "\n",
    "def f(x):\n",
    "    return np.cos(x) + np.sqrt(1 + np.sin(np.tanh(x))) / 3\n",
    "\n",
    "\n",
    "def gNewton(func, x0, tol, maxits=1E3):\n",
    "    assert tol>0\n",
    "    Dfunc = value_and_grad(func)\n",
    "    \n",
    "    its = 1\n",
    "    y, dy_dx = Dfunc(x0)\n",
    "    x1 = x0 - y/dy_dx\n",
    "    \n",
    "    while abs(x1 - x0) > tol and its < maxits:\n",
    "        x0 = x1\n",
    "        its  += 1\n",
    "        y, dy_dx = Dfunc(x1)\n",
    "        x1 = x1 - y/dy_dx\n",
    "    \n",
    "    return x1, its\n",
    "\n",
    "for x_init in [-2.0, 2.0, 4.0, 8.0]:\n",
    "    print(f\"{x_init}: {gNewton(f, x_init, 1E-10)}\")\n",
    "    \n",
    "# Fyll in kode for gNewton her\n",
    "\n",
    "# Utfør påkrevde tester under her\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oppgave 3** Newtons metode for systemer av ligninger\n",
    "\n",
    "Nå skal vi gå litt videre i forhold til forelesningsmaterialet, og se på systemer av ligninger.\n",
    "Newton's metode kan fremdeles defineres, men må generaliseres. Et $n$-dimensjonalt system av ligninger kan skrives\n",
    "$$\n",
    "\\begin{array}{lcr}\n",
    "f_0(x_0,\\ldots,x_{n-1})&=&0\\\\\n",
    "f_1(x_0,\\ldots,x_{n-1})&=&0\\\\\n",
    "&\\vdots&\\\\\n",
    "f_{n-1}(x_0,\\ldots,x_{n-1})&=&0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "dvs $n$ ligninger $f_0,\\ldots,f_{n-1}$ for $n$ ukjente $x_0,\\ldots,x_{n-1}$. \n",
    "I kortspråk skriver vi dette kun som $\\mathbf{f}(\\mathbf{x})=0$.\n",
    "\n",
    "En sentral størrelse her er den såkalte\n",
    "Jacobi-matrisen. Hvis vi partiellderiverer $n$ funksjoner, hver med hensyn på $n$ variable, så blir det totalt\n",
    "$n^2$ funksjoner, dette er elementene i Jacobimatrisen\n",
    "\n",
    "$$\n",
    "D\\mathbf{f} = \\left[\n",
    "\\begin{array}{cccc}\n",
    "\\frac{\\partial f_0}{\\partial x_0} & \\frac{\\partial f_0}{\\partial x_1} & \\cdots & \\frac{\\partial f_0}{\\partial x_{n-1}} \\\\\n",
    "\\frac{\\partial f_1}{\\partial x_0} & \\frac{\\partial f_1}{\\partial x_1} &\\cdots & \\frac{\\partial f_1}{\\partial x_{n-1}}\\\\\n",
    "\\vdots                            & \\vdots                            &        &    \\vdots   \\\\\n",
    "\\frac{\\partial f_{n-1}}{\\partial x_0} & \\frac{\\partial f_{n-1}}{\\partial x_1} &\\cdots & \\frac{\\partial f_{n-1}}{\\partial x_{n-1}}\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Generaliseringen av Newton's metode er nå rett og slett: Gitt $\\mathbf{x}^{(0)}\\in\\mathbb{R}^n$ la\n",
    "\n",
    "$$\n",
    "    \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\left[D\\mathbf{f}(\\mathbf{x}^{(k)})\\right]^{-1}\\cdot \\mathbf{f}(\\mathbf{x}^{(k)})\n",
    "$$\n",
    "\n",
    "For å implementere denne kan vi definere $\\mathbf{f}$ som en funksjon som tar et numpy-array som input og returnerer et numpy-array. Så fins det en funksjon i pakken *autograd* som heter *jacobian* og som beregner Jacobimatrisen. Vi illustrerer hvordan vi bruker *jacobian* på funksjonen \n",
    "\n",
    "$$\n",
    "\\mathbf{f}(x_0,x_1) = \\left[\\begin{array}{c}x_0^2+x_1^2\\\\ x_0 x_1\\end{array}\\right]\\quad\\Rightarrow\\quad \n",
    "Df=\\left[\n",
    "\\begin{array}{cc}\n",
    "2x_0 & 2 x_1 \\\\ x_1 & x_0\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 4.]\n",
      " [2. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import jacobian\n",
    "\n",
    "def f(x):\n",
    "    return np.array([x[0]**2+x[1]**2,x[0]*x[1]])\n",
    "\n",
    "Df = jacobian(f)\n",
    "x=np.array([1.0,2.0])\n",
    "print(Df(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merk at kallet til *jacobian* gjøres bare én gang når *f* er definert. Etterpå kan *Df* brukes flere ganger med ulike inputargumenter.\n",
    "\n",
    "Nå litt om hvordan man implementerer Newton's metode. Inversmatrisen som figurere i iterasjonen gjør vi her enkelt ved å kalle en ferdigrutine i *numpy.linalg*, se nedenfor. Hvis vi definerer $\\delta=D\\mathbf{f}(\\mathbf{x}^{(k)})^{-1}\\cdot\\mathbf{f}(\\mathbf{x}^{(k)})$ kan vi skrive en Newtoniterasjon som\n",
    "1. Løs $D\\mathbf{f}(\\mathbf{x}^{(k)})\\delta = -\\mathbf{f}(\\mathbf{x}^{(k)})$ m.h.p. $\\delta$\n",
    "2. Sett $\\mathbf{x}^{(k+1)}=\\mathbf{x}^{(k)}+\\delta$\n",
    "\n",
    "I *numpy* fins en delpakke som heter *numpy.linalg* og i denne fins en funksjon som heter *solve*. Derfor kan vi løse ligningssystemer som i 1. ved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "\n",
    "A=np.array([[2,1],[3,1]])\n",
    "b=np.array([1,1])\n",
    "y=la.solve(A,b)\n",
    "print(y,la.norm(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi har også tatt med *la.norm* som beregner lengden til en vektor. Det trenger vi å gjøre når vi skal lage stoppkriterium i Newtoniterasjonen, det er naturlig å kreve at $\\mathrm{la.norm(delta)}<\\mathrm{tol}$.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**(a)** Skriv en funksjon sNewton som tar tre inputargumenter: funksjonen $\\mathbf{f}(\\mathbf{x})$ som en skal finne nullpunkter til, startverdien $\\mathbf{x}^{(0)}$, og toleransen tol. Returner approksimert rot og antall iterasjoner.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**(b)** Test ut sNewton på å finne nullpunkter til systemet\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl}\n",
    "x_0^2 + x_1^2 &=& 1 \\\\\n",
    "x_0^3 - x_1 &=& 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "idet du velger toleransen slik at du kan gi et pålitelig svar på Kontrollspørsmål 3. Den første ligningen sier at eventuelle nullpunkter ligger på sirkelen sentrert i $(0,0)$ med radius 1. Den kubiske kurven $x_1=x_0^3$ fra den andre ligningen skjærer denne sirkelen nøyaktig to steder, så det fins akkurat 2 nullpunkter.\n",
    "</div>\n",
    "\n",
    "**Kontrollspørsmål 3.** I spørsmål **(3b)** angi det åttende desimalet etter komma for hhv $x_0$- og $x_1$-komponenten av roten (merk at det spiller ingen rolle hvilken av de to røttene du velger). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0: -0.8260313577\n",
      "x1: -0.5636241622\n",
      "its: 13\n"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import jacobian \n",
    "import numpy.linalg as la\n",
    "\n",
    "def f(x):\n",
    "    # assert x.shape == tuple([2])\n",
    "    ret = np.array([\n",
    "        x[0]**2 + x[1]**2 - 1,\n",
    "        x[0]**3 - x[1]\n",
    "    ])\n",
    "    return ret\n",
    "\n",
    "def sNewton(func,x0,tol, maxiter=1E3):\n",
    "    #Skriv kode for Newtoniterasjoner for systemer med automatisk derivasjon her\n",
    "    D_f = jacobian(f)\n",
    "    delta = lambda x: la.solve(D_f(x), f(x))  # I've chosen to define delta as D_f^-1 * f\n",
    "    x1 = x0 - delta(x0)\n",
    "    \n",
    "    its = 1\n",
    "    while la.norm(x1 - x0) and its < maxiter:\n",
    "        its += 1\n",
    "        x0 = x1\n",
    "        x1 = x0 - delta(x0)\n",
    " \n",
    "    return x1, its\n",
    "\n",
    "res, its = sNewton(f, np.array([0.1, 0], dtype=float), 1E11)\n",
    "\n",
    "print(f\"x0: {res[0]:.10f}\\nx1: {res[1]:.10f}\\nits: {its}\")\n",
    "\n",
    "\n",
    "#Gjør kall til sNewton og utfør påkrevde eksperimenter under her \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
